\chapter{Einleitung}\label{einleitung}

Die Hardware"=Konfigurationen moderner Computersysteme sind in den letzten
Jahren deutlich heterogener geworden. Mobiltelefone und PCs verfügen nicht nur
über mehrkernige, leistungsstarke Prozessoren (\gls{cpu}), sondern auch über
dedizierte Chips zur Grafikherstellung (\gls{gpu}). Dagegen verschwimmt bei
Spielekonsolen und Laptops die Unterscheidung zwischen Haupt- und
Grafikprozessoren in Form integrierter Chips, die beide Funktionalitäten
performant abdecken können (\gls{apu}). Im Bereich des \gls{hpc} sind
verschiedene Beschleunigertypen seit jeher im Einsatz, seien es CPUs mit
besonders vielen Kernen, von den GPUs abstammende Beschleuniger ohne grafische
Ausgabe wie NVIDIAs Tesla"=Reihe, oder Hybride wie Intels
kurzlebiges Xeon"=Phi"=Projekt.

Allen gemeinsam ist die Idee der Beschleunigung eines Algorithmus durch dessen
Parallelisierung, also die parallele Ausführung einzelner Schleifeniterationen
auf den in den Beschleunigern vorhandenen Kernen, Multiprozessoren usw. Wie eine
CPU sind die Beschleuniger allerdings darauf ausgelegt, von verschiedenen
Algorithmen genutzt werden zu können, und entsprechend allgemein aufgebaut. Der
Programmierer findet also möglicherweise eine Hardware"=Plattform vor, die für
seinen Algorithmus nicht ideal geeignet ist.

In dieses Feld stoßen seit einigen Jahren die Hersteller Intel und Xilinx mit
programmierbarer Hardware (\gls{fpga}) vor. Gegenüber den oben genannten
Beschleunigern haben diese den Vorteil, dass die einzelnen Bestandteile der
Hardware weitestgehend frei verschaltbar sind, wenngleich mit deutlich höherem
Zeitaufwand. Dies ist vor allem für Programmierer interessant, die die Hardware
für genau einen Zweck verwenden möchten. Lange Zeit stand dem jedoch die
vergleichsweise schwierige Verwendung der FPGAs entgegen, da diese einen
Schaltungsentwurf auf der Register"=Ebene (im Gegensatz zur Programmierung auf
der algorithmischen Ebene) benötigten. Durch das Aufkommen automatischer
Synthese"=Werkzeuge, die Algorithmen in Schaltungen umwandeln können, wurde der
Entwicklungsprozess in jüngerer Zeit allerdings deutlich vereinfacht.

Ein weiteres Hindernis für die Entwicklung paralleler Programme ist die
fehlende Portabilität der Werkzeuge: anfangs waren nur GPUs des Herstellers
NVIDIA ohne Umwege über Schnittstellen zur Grafikdarstellung programmierbar,
erforderten dafür aber die Nutzung der NVIDIA"=eigenen CUDA"=Schnittstelle, die
mit Konkurrenzprodukten nicht kompatibel war. Für CPUs etablierte sich schnell
die OpenMP"=Schnittstelle als Mittel der Wahl für die Nutzung mehrerer Kerne.
Die Vektorisierung hingegen erfordert noch immer entweder befehlssatzspezifische
Erweiterungen (die auch innerhalb derselben Befehlssatzfamilie mitunter
inkompatibel sind) oder sehr gut optimierende Compiler. Die Synthese"=Werkzeuge
der FPGA"=Hersteller ermöglichten zwar die Nutzung der Hochsprachen, reicherten
diese aber mit zahlreichen Erweiterungen an, was die Übertragung auf andere
Plattformen erschwerte.

Um die Entwicklung portabler Programme zu ermöglichen, wurden früh verschiedene
Ansätze entwickelt. Das Khronos"=Industriekonsortium gab wenige Jahre nach der
ersten CUDA"=Veröffentlichung die Spezifikation der OpenCL"=Schnittstelle
heraus. Dahinter verbarg sich die Idee, eine einheitliche Schnittstelle für den
Programmierer zu schaffen, die im Hintergrund von jedem Hardware"=Hersteller für
die eigenen Produkte implementiert und optimiert wird. Die OpenCL"=Entwicklung
wurde anfangs von zahlreichen namhaften Hard- und Software"=Herstellern
unterstützt (z.B. Apple, AMD, NVIDIA, Intel und Xilinx), flachte jedoch nach
wenigen Jahren wieder ab und konnte sich in der GPU"=Welt nicht gegen CUDA
und auf CPUs nicht gegen OpenMP durchsetzen.

Ein anderer Ansatz liegt in der Entwicklung eines abstrakten Interfaces, das im
Hintergrund auf die herstellereigenen Schnittstellen abgebildet wird. Ein solches
Interface transformiert einen vom Programmierer entwickelten Quelltext in einen
äquivalenten CUDA- oder OpenMP"=Quelltext, ohne dass der Programmierer selbst
weitere Anstrengungen in dieser Richtung unternehmen muss. So genügt ein
einfacher Austausch der Zielplattform und eine erneute Kompilierung für die
Nutzung eines anderen Hardware"=Typs. Dieses Prinzip wird von mehreren
parallelen Projekten verfolgt, wie etwa der vom Helmholtz"=Zentrum
Dresden"=Rossendorf entwickelten Alpaka"=Bibliothek oder der Kokkos"=Bibliothek,
die von einer Forschungseinrichtung des US"=amerikanischen Energieministeriums
stammt. Bisher bieten jedoch weder Alpaka noch Kokkos eine Unterstützung für
FPGAs an.

Seit einigen Jahren versucht das Khronos"=Konsortium erneut, einen Standard für
die parallele Programmierung zu etablieren. Dieser SYCL genannte Ansatz basiert
auf dem einige Jahre älteren OpenCL, bietet aber eine deutlich modernere und
fortschrittlichere Programmierschnittstelle. Der neue Standard wird unter
anderem von den FPGA"=Herstellern Xilinx und Intel vorangetrieben und ist damit
ein interessantes Implementierungsziel für die oben genannten
Abstraktionsbibliotheken.

\section{Forschungsstand}\label{einleitung:forschung}

In den letzten Jahren befassten sich mehrere Forschungsgruppen mit der
automatischen \mbox{FPGA}"=Synthese für hochparallele Programme.

Schon 2009 veröffentlichten Papakonstantinou \textit{et al.} einen Ansatz, der
die Synthese einer Schaltung auf Basis der eigentlich für GPUs gedachten
CUDA"=Schnittstelle ermöglichte. \cite[vgl.][]{papakonstantinou2009} 

Diese Arbeit konnte sich jedoch nicht langfristig durchsetzen. Seit der ersten
Veröffentlichung einer OpenCL"=Implementierung für FPGAs durch den Hersteller
Altera (heute Intel) im Jahr 2013 verlagerte sich das Interesse der Forschung
auf diese Plattform. 

Eine der ersten Arbeiten in diesem Bereich wurde 2013 von Settle, einem
damaligen Altera"=Mitarbeiter, veröffentlicht. In ihr zeigte Settle den durch
eine abstrakte Sprache wie OpenCL zu erreichenden Produktivitätsgewinn bei
gleichzeitiger Beibehaltung der erreichten Leistung. \cite[vgl.][]{settle2013}

Fifield \textit{et al.} demonstrierten 2016 die Optimierung von
OpenCL"=Programmen für die im Vorjahr veröffentlichte
Xilinx"=OpenCL"=Implementierung. \cite[vgl.][]{fifield2016}

Duarte \textit{et al.} stellten 2018 das \textit{hls4ml}"=Projekt vor. Dabei
handelt es sich um Implementierungen neuraler Netzwerke, die durch automatische
Synthese in Schaltungen für Xilinx"=FPGAs umgewandelt werden. In diesem Projekt
kommt allerdings nicht OpenCL zum Einsatz, sondern die Xilinx"=Erweiterungen für
die Programmiersprache C++. \cite[vgl.][]{duarte2018}

Die SYCL"=Spezifikation wurde in der Literatur in den ersten Jahren ihres
Bestehens vornehmlich als einfacher Aufsatz für OpenCL betrachtet. Erst in
jüngerer Zeit kam es zu eigenständigen Untersuchungen SYCLs.

Eine der frühen Arbeiten, die sich von dieser unscharfen Betrachtungsweise
abhebt, ist Žužeks Masterarbeit aus dem Jahre 2016, in deren Rahmen eine
eigenständige \gls{sycl}"=Implementierung entwickelt wurde.
\cite[vgl.][]{zuzek2016}

Wong \textit{et al.}, Mitarbeiter der Firma Codeplay -- einer der führenden
Firmen im SYCL"=Umfeld --, befassten sich ebenfalls 2016 mit den
Wechselwirkungen zwischen dem C++"=Standard und der auf diesem Standard
aufbauenden SYCL"=Spezifikation. Aufmerksamkeit wurde insbesondere den bei der
Codeplay"=SYCL"=Implementierung aufgetretenen Probleme sowie Unzulänglichkeiten
des C++"=Standards zuteil.
\cite[vgl.][]{wong2016}

Aliaga \textit{et al.} veröffentlichten 2017 eine in C++ und SYCL geschriebene
Implementierung der BLAS"=Schnittstelle, die sie \textit{SYCL"=BLAS} nannten.
\cite[vgl.][]{aliaga2017}

Burns \textit{et al.} stellten 2019 das \textit{SYCL"=DNN}"=Projekt vor, eine in
C++ und SYCL geschriebene Bibliothek für die Beschleunigung von Operationen, die
typischerweise in neuralen Netzwerken verwendet werden. Teil der Arbeit war auch
ein Vergleich mit den konkurrierenden Bibliotheken cuDNN (NVIDIA) und
MIOpen (AMD). Im Gegensatz zu diesen herstellerspezifischen Bibliotheken soll
SYCL"=DNN auf einer Vielzahl OpenCL"=fähiger Beschleuniger lauffähig sein.
\cite[vgl.][]{burns2019}

Hinsichtlich der Verwendung von SYCL auf FPGAs ist in der Literatur -- neben
einigen weitestgehend inhaltsgleichen Vorträgen des Xilinx"=Mitarbeiters Ronan
Keryell -- bisher nur der 2017 von Doumoulakis \textit{et al.} veröffentlichte
Artikel zu finden, der sich mit der Interoperabilität von SYCL und OpenCL auf
Xilinx"=FPGAs befasst.
\cite[vgl.][]{doumoulakis2017}

Als Backend für Abstraktionsbibliotheken wie Alpaka oder Kokkos fand SYCL bisher
keine Verwendung. Zwar veröffentlichten Copik \textit{et al.} 2017 einen Artikel
über die experimentelle Implementierung eines solchen Backends für die Kokkos
und Alpaka sehr ähnliche Bibliothek \textit{HPX.Compute}, bis heute ist dieser
Entwicklungszweig aber nicht in das Hauptprojekt aufgenommen worden.
\cite[vgl.][]{copik2017}

Im Zusammenhang mit Kokkos findet SYCL bisher nur in Form einer von Hammond
\textit{et al.} 2019 durchgeführten Studie Erwähnung. Dabei handelt es sich
jedoch um einen Vergleich der Programmiermodelle von Kokkos und SYCL und nicht
um eine Implementierung eines Kokkos"=Backends.
\cite[vgl.][]{hammond2019}

\section{Ziel der Arbeit}\label{einleitung:ziel}

In dieser Arbeit soll für die Alpaka-Bibliothek ein SYCL-Backend implementiert
und ausgewertet werden. Aufgrund der bereits vorhandenen \gls{gpu}-Backends
erfolgt die Analyse vorrangig im Hinblick auf die Nutzbarkeit von \gls{fpga}s,
womit aufgrund der während des Bearbeitungszeitruams verfügbaren und von SYCL
unterstützten Hardware insbesondere \gls{fpga}s des Herstellers Xilinx gemeint
sind.
