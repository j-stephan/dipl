\chapter{Einleitung}\label{einleitung}

Die Hardware"=Ausstattungen moderner Computersysteme werden zunehmend heterogen.
Mobiltelefone und PCs verfügen nicht nur über mehrkernige, leistungsstarke
Prozessoren (\gls{cpu}), sondern auch über dedizierte Chips zur
Grafikdarstellung (\gls{gpu}). Währenddessen verschwimmt bei Spielekonsolen und
Laptops die Unterscheidung zwischen Haupt- und Grafikprozessoren dank
integrierter Chips, die beide Funktionalitäten performant abdecken können
(\gls{apu}). GPUs unterscheiden sich von CPUs vor allem durch einen
spezifischeren Einsatzzweck: Während CPUs vielfältige verschiedene
Anwendungen abdecken und diese parallel ausführen können (Taskparallelität),
wurden GPUs vornehmlich für die möglichst schnelle und parallele Berechnung
einer hohen Zahl von Bildpunkten konzipiert. Datenparallele Algorithmen -- also
Algorithmen, die parallel auf viele gleichartige Daten angewendet werden -- sind
deshalb gut dafür geeignet, auf GPUs ausgelagert und von diesen berechnet zu
werden. Anders ausgedrückt lassen sich GPUs als \textit{Beschleuniger} für
datenparallele Algorithmen verwenden. Im Bereich des \gls{hpc} sind verschiedene
Beschleunigertypen seit jeher im Einsatz, seien es CPUs mit besonders vielen
Rechenkernen, von den GPUs abstammende Beschleuniger ohne grafische Ausgabe wie
NVIDIAs Tesla"=Reihe oder Hybride wie Intels Xeon"=Phi"=Produkte.

Allen gemeinsam ist die Idee der Beschleunigung eines Algorithmus durch dessen
Parallelisierung, also die in der Regel parallele Ausführung einzelner
Schleifeniterationen auf den in den Beschleunigern vorhandenen Rechenkernen.
Wie eine CPU sind die Beschleuniger allerdings darauf ausgelegt, von
verschiedenen Algorithmen genutzt werden zu können und entsprechend allgemein
aufgebaut. Der Programmierer findet also möglicherweise eine Hardware"=Plattform
vor, die für seinen Algorithmus nicht ideal geeignet ist.

Einen Chip, der für einen speziellen Algorithmus entworfen wurde, bezeichnet man
als \gls{asic}. ASICs erfordern wegen ihrer Produktionskosten einen hohen
finanziellen Aufwand und sind daher erst ab hohen Stückzahlen interessant.
Darüber hinaus bergen sie das Risiko, geänderten Einsatzzwecken -- z.B. durch
beim Schaltungsentwurf nicht bedachte Aspekte oder später geänderte
Anforderungen -- nicht mehr zu genügen, was wiederum eine aufwendige
Neuentwicklung und -produktion nötig macht. Im schlimmsten Fall ist der zu
tauschende ASIC für einen Menschen physisch nicht mehr zu erreichen, wie etwa in
der Raumfahrt oder bei Detektoren innerhalb eines in Betrieb befindlichen
Teilchenbeschleunigers.

Durch dynamisch rekonfigurierbare bzw. programmierbare Hardware lassen sich die
beschriebenen Herausforderungen besser meistern -- wenn auch mit dem Nachteil,
gegenüber spezialisierten Chips deutlich langsamer zu sein. Auf diesem Gebiet
ist vor allem der Hardware"=Typ \gls{fpga} zu nennen. Gegenüber den oben
genannten Beschleunigern und spezialisierten Chips haben FPGAs den Vorteil, dass
die einzelnen Chip"=Bestandteile -- also Logikfunktionen, On"=Chip"=Speicher
usw. -- weitestgehend frei verschaltbar sind. 
Lange Zeit stand dem jedoch die vergleichsweise schwierige Verwendung der
FPGAs entgegen, da diese einen Schaltungsentwurf auf der Register"=Ebene (im
Gegensatz zur Programmierung auf der algorithmischen Ebene) benötigten. Durch
das Aufkommen automatischer Werkzeuge für die \gls{hls}, die Algorithmen in
Schaltungen umwandeln können, wurde der Entwicklungsprozess in jüngerer Zeit
aber deutlich vereinfacht.

Dadurch sind FPGAs auch für Anwendungen abseits des klassischen
Schaltungsentwurfs interessant. Insbesondere latenzkritische Anwendungen eignen
sich gut für den Einsatz von FPGAs, da der Zeitpunkt der Ergebnisausgabe
taktgenau vorhersagbar ist. Die Inferenz neuronaler Netzwerke oder die
Verarbeitung großer Datenströme im \gls{hpc}"=Bereich sind Beispiele für
\textit{stream}"=artige Probleme, die vom Einsatz eines FPGAs profitieren
können.

Darüber hinaus sind FPGAs gegenüber GPUs noch in anderen Punkten flexibler. Da
Schaltungen weitgehend frei entworfen werden können, lassen sich Schnittstellen
zu externer Hardware mit deutlich kürzeren Kommunikationswegen entwerfen, als
dies für GPUs möglich wäre. Kürzere Kommunikationswege bedeuten auch eine
geringere Störanfälligkeit gegenüber Strahlung, was FPGAs z.B. für den Einsatz
in Experimenten der Teilchenphysik interessant macht. Auch der bei GPUs durch
das Betriebssystem und die Hardware"=Treiber vorhandene Overhead lässt sich bei
FPGAs vermeiden -- sofern keine Abhängigkeit des Algorithmus oder weiterer
Hardware"=Komponenten zu Software besteht --, da die Schaltung selbst von
Software unabhängig ist.

Da die Synthese einer Schaltung je nach Komplexität des Algorithmus einige
Stunden bis Tage benötigen kann, ist für das \textit{Prototyping} ratsam,
schnellere Methoden für die inkrementelle Entwicklung zu wählen -- also z.B.
Beschleuniger. Wünschenswert ist hier vor allem Portabilität zwischen den
verschiedenen Hardware"=Plattformen.

Das ist durchaus nicht unproblematisch: So waren in den Anfangsjahren des
\gls{gpgpu} nur die Produkte des Herstellers NVIDIA ohne Umwege über
Grafikschnittstellen programmierbar, erforderten dafür aber die Nutzung des
erstmals 2006 erschienenen NVIDIA"=eigenen CUDA"=API. Dieses war (und ist)
jedoch nicht mit Konkurrenzprodukten kompatibel. Für CPUs etablierte sich
schnell die seit 1997 entwickelte Schnittstelle \textit{Open Multi-Processing}
(\gls{openmp}) als Mittel der Wahl für die Nutzung mehrerer Rechenkerne. Die
Vektorisierung erfordert dagegen entweder befehlssatzspezifische Erweiterungen
-- die auch innerhalb derselben Befehlssatzfamilie mitunter inkompatibel sind --
oder sehr gut optimierende Compiler. Die seit 2011 verfügbaren HLS"=Werkzeuge
des FPGA"=Herstellers Xilinx ermöglichen zwar die Nutzung der Hochsprachen,
reichern diese aber mit zahlreichen Erweiterungen an, was die Übertragung
zwischen den Plattformen erschwert.

Um die Entwicklung portabler Programme zu ermöglichen, wurden früh verschiedene
Ansätze entwickelt. Das Khronos"=Industriekonsortium gab 2008 die Spezifikation
der \textit{Open Computing Language} (\gls{opencl}) heraus. Dahinter verbarg sich
die Idee, eine einheitliche Schnittstelle für den Programmierer zu schaffen, die
im Hintergrund von jedem Hardware"=Hersteller für die eigenen Produkte
implementiert und optimiert wird. Die OpenCL"=Entwicklung wurde anfangs von
zahlreichen namhaften Hard"= und Software"=Herstellern unterstützt (z.B. Apple,
AMD, NVIDIA, Intel, Altera und Xilinx), flachte jedoch nach wenigen Jahren
wieder ab und konnte sich auf GPUs nicht gegen \gls{cuda} und auf CPUs nicht
gegen \gls{openmp} durchsetzen.

Ein anderer Ansatz liegt in der Entwicklung eines abstrakten Interfaces, das im
Hintergrund auf die herstellereigenen Schnittstellen abgebildet wird. Ein
solches Interface transformiert einen vom Programmierer entwickelten Quelltext
z.B. in einen äquivalenten CUDA"=Quelltext, ohne dass der Programmierer selbst
weitere Anstrengungen in dieser Richtung unternehmen muss. So genügt ein
einfacher Austausch der Zielplattform und eine erneute Kompilierung für die
Nutzung eines anderen Hardware"=Typs. Dieses Prinzip wird parallel von mehreren
Projekten verfolgt, wie etwa von der vom Helmholtz"=Zentrum Dresden"=Rossendorf
entwickelten Alpaka"=Bibliothek oder der Kokkos"=Bibliothek,
die von einer Forschungseinrichtung des US"=amerikanischen Energieministeriums
stammt. Bisher bieten jedoch weder Alpaka noch Kokkos ein Backend für FPGAs an.

Seit einigen Jahren versucht das Khronos"=Konsortium erneut, einen Standard für
die parallele Programmierung zu etablieren. Dieser SYCL genannte Ansatz basiert
auf dem einige Jahre älteren OpenCL, bietet aber eine deutlich modernere und
fortschrittlichere C++"=Programmierschnittstelle. Der neue Standard wird unter
anderem von den FPGA"=Herstellern Xilinx und Intel vorangetrieben und wäre damit
eine interessante Backend"=Variante für die oben genannten
Abstraktionsbibliotheken.

\section{Forschungsstand}\label{einleitung:forschung}

In den letzten Jahren befassten sich mehrere Forschungsgruppen mit der
automatisierten \mbox{FPGA}"=Synthese für hochparallele Programme.

Schon 2009 veröffentlichten Papakonstantinou \textit{et al.} einen Ansatz, der
die Synthese einer Schaltung auf Basis der eigentlich für NVIDIA"=GPUs gedachten
CUDA"=Schnittstelle ermöglichte. \cite[vgl.][]{papakonstantinou2009} 

Diese Arbeit konnte sich jedoch nicht langfristig durchsetzen. Seit der ersten
Veröffentlichung einer OpenCL"=Implementierung für FPGAs durch den Hersteller
Altera (heute Intel) im Jahr 2013 verlagerte sich das Interesse der Forschung
auf diese Plattform. 

Eine der ersten Arbeiten in diesem Bereich wurde 2013 von Settle, einem
damaligen Altera"=Mitarbeiter, veröffentlicht. In ihr zeigte Settle den
-- gegenüber der Entwicklung auf Registerebene -- durch eine Hochsprache wie
OpenCL zu erreichenden Produktivitätsgewinn bei gleichzeitiger Beibehaltung der
erreichten Leistung. \cite[vgl.][]{settle2013}

Fifield \textit{et al.} demonstrierten 2016 die Optimierung von
OpenCL"=Programmen für die im Vorjahr veröffentlichte
Xilinx"=OpenCL"=Implementierung. \cite[vgl.][]{fifield2016}

Duarte \textit{et al.} stellten 2018 das \textit{hls4ml}"=Projekt vor. Dabei
handelt es sich um Implementierungen neuronaler Netzwerke, die durch
automatische Synthese in Schaltungen für Xilinx"=FPGAs umgewandelt werden. In
diesem Projekt kommt allerdings nicht OpenCL zum Einsatz. Stattdessen werden
die Xilinx"=Erweiterungen für die Programmiersprache C++ verwendet.
\cite[vgl.][]{duarte2018}

Die SYCL"=Spezifikation wurde in der Literatur in den ersten Jahren ihres
Bestehens vornehmlich als einfacher Überbau für OpenCL betrachtet. Erst in
jüngerer Zeit kam es zu eigenständigen Untersuchungen SYCLs.

Eine der frühen Arbeiten, die sich von dieser unscharfen Betrachtungsweise
abhebt, ist Žužeks Masterarbeit aus dem Jahre 2016, in deren Rahmen eine
eigenständige \gls{sycl}"=Implementierung entwickelt wurde.
\cite[vgl.][]{zuzek2016}

Wong \textit{et al.}, Mitarbeiter der Firma Codeplay -- einer der führenden
Firmen im SYCL"=Umfeld --, befassten sich ebenfalls 2016 mit den
Wechselwirkungen zwischen dem C++"=Standard und der auf diesem Standard
aufbauenden SYCL"=Spezifikation. Aufmerksamkeit wurde insbesondere den bei der
Codeplay"=SYCL"=Implementierung aufgetretenen Problemen sowie Unzulänglichkeiten
des C++"=Standards zuteil.
\cite[vgl.][]{wong2016}

Aliaga \textit{et al.} veröffentlichten 2017 eine in C++ und SYCL geschriebene
Implementierung der BLAS"=Schnittstelle -- ein Quasistandard für
Rechenoperationen der linearen Algebra --, die sie \textit{SYCL"=BLAS} nannten.
\cite[vgl.][]{aliaga2017}

Burns \textit{et al.} stellten 2019 das \textit{SYCL"=DNN}"=Projekt vor, eine in
C++ und SYCL geschriebene Bibliothek für die Beschleunigung von Operationen, die
typischerweise in neuronalen Netzwerken verwendet werden. Teil der Arbeit war
auch ein Vergleich mit den konkurrierenden Bibliotheken cuDNN (NVIDIA) und
MIOpen (AMD). Im Gegensatz zu diesen herstellerspezifischen Bibliotheken soll
SYCL"=DNN auf einer Vielzahl OpenCL"=fähiger Beschleuniger lauffähig sein.
\cite[vgl.][]{burns2019}

Hinsichtlich der Verwendung von SYCL auf FPGAs ist in der Literatur -- neben
einigen untereinander recht ähnlichen Vorträgen des Xilinx"=Mitarbeiters Ronan
Keryell -- bisher nur der 2017 von Doumoulakis \textit{et al.} veröffentlichte
Artikel zu finden, der sich mit der Interoperabilität von SYCL und OpenCL auf
Xilinx"=FPGAs befasst.
\cite[vgl.][]{doumoulakis2017}

Als Backend für Abstraktionsbibliotheken wie Alpaka oder Kokkos fand SYCL
bislang keine Verwendung. Zwar veröffentlichten Copik \textit{et al.} 2017 einen
Artikel über die experimentelle Implementierung eines solchen Backends für die
Kokkos und Alpaka sehr ähnliche Bibliothek \textit{HPX.Compute}, bis heute ist
dieser Entwicklungszweig aber nicht in das Hauptprojekt aufgenommen worden.
\cite[vgl.][]{copik2017}

Im Zusammenhang mit Kokkos findet SYCL bisher nur in Form einer von Hammond
\textit{et al.} 2019 durchgeführten Studie Erwähnung. Dabei handelt es sich
jedoch um einen Vergleich der Programmiermodelle von Kokkos und SYCL und nicht
um eine Implementierung eines Kokkos"=Backends.
\cite[vgl.][]{hammond2019}

\section{Ziel der Arbeit}\label{einleitung:ziel}

In dieser Arbeit wird der SYCL"=Standard hinsichtlich der verfügbaren
Implementierungen und deren Nutzbarkeit untersucht. Dies geschieht vor allem im
Hinblick auf FPGAs, die durch ihre frei veränderbare Hardware"=Konfiguration in
Verbindung mit einer modernen C++"=Programmierschnittstelle eine
vielversprechende Hardware"=Plattform darstellen. Dabei wird ein
experimentelles SYCL"=Backend für die Alpaka-Bibliothek implementiert. Die
während des Entwicklungsprozesses aufgetretenen Schwierigkeiten und
Unzulänglichkeiten werden analysiert sowie erste Einschätzungen der
Leistungsfähigkeit vorgenommen. Die Verifizierung des SYCL"=Backends erfolgt
anhand einer bereits existierenden und mit Alpaka parallelisierten Anwendung aus
dem Umfeld der Laser"=Teilchenbeschleunigung.
