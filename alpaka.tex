\chapter{Die Alpaka-Bibliothek}\label{alpaka}

Dieses Kapitel führt in die Alpaka"=Bibliothek ein. Wie im vorherigen
SYCL"=Kapitel wird der grundlegende Aufbau eines Alpaka"=Programms anhand des
AXPY"=Beispiels dargestellt. Ein weiterer Abschnitt ist den darauf aufbauenden,
erweiterten Konzepten, wie etwa der Hardware"=Abstraktion, gewidmet.

\section{Überblick}\label{alpaka:ueberblick}

Alpaka (Eigenschreibweise: \textit{alpaka}) steht für
\textit{Abstraction Library for Parallel Kernel Acceleration} und wurde
ursprünglich von Benjamin Worpitz im Rahmen seiner Masterarbeit entwickelt
\cite[vgl.][]{worpitz2015}. Mittlerweile wird die Entwicklung durch
die Gruppe \textit{Computergestützte Strahlenphysik} des
\textit{Instituts für Strahlenphysik} am
\textit{Helmholtz"=Zentrum Dresden"=Rossendorf} fortgeführt.

Die Alpaka"=Bibliothek definiert eine abstrakte C++"=Schnittstelle, mit deren
Hilfe parallele Programme geschrieben werden können. Im Hintergrund wird Alpaka
auf hersteller- oder hardware"=spezifische Schnittstellen, wie CUDA oder OpenMP,
-- im Folgenden als \textit{Backend} bezeichnet -- abgebildet. Alpaka ist somit
ein einheitliches Paket, das die abstrakte Schnittstelle nach außen und die
konkrete Implementierung vereinigt. Damit unterscheidet sich die Bibliothek von
ähnlichen Ansätzen wie OpenCL oder SYCL, die ebenfalls eine abstrakte
Schnittstelle definieren, die Implementierung jedoch den Hardware- und
Software"=Herstellern überlassen.

Wie bei SYCL sind die Quelltexte für \textit{Host} und \textit{Device} nicht
voneinander getrennt. Die Abbildung auf ein oder mehrere Backends erfolgt zur
Compile"=Zeit durch Template"=Metaprogrammierung, wodurch ein
Abstraktions"=Overhead zur Laufzeit vermieden wird.

Ähnlich wie SYCL bietet auch Alpaka ein beschleunigerunabhängiges Backend, das
sich prinzipiell mit jedem modernen C++"=Compiler kompilieren lässt. Dieses
wird in der Alpaka"=Terminologie \texttt{CpuSerial} genannt und führt jeden
Befehl seriell aus. Während der Kernel"=Ausführung existiert also keinerlei
Parallelität. Daneben gibt es weitere Implementierungen für CPUs auf Basis des
OpenMP"=Standards, der ebenfalls von den meisten modernen C++"=Compilern
unterstützt wird.

\subsection{AXPY und Alpaka}\label{alpaka:ueberblick:axpy}

Zum Zwecke der einfachen Vergleichbarkeit der von SYCL und Alpaka gebotenen
Programmiermodelle wird das im vorigen Kapitel verwendete AXPY"=Beispiel der
BLAS"=Bibliothek hier erneut aufgegriffen. Strukturell ähneln sich SYCL und
Alpaka recht stark, wie der
Platzhalter"=Quelltext~\ref{alpaka:ueberblick:axpy:struktur} zeigt. Wie im
vorigen Kapitel wird auch dieser Quelltext nach und nach mit Inhalt gefüllt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
#include <cstdlib>

#include <alpaka/alpaka.hpp>

auto main() -> int
{
    // Beschleunigerwahl und Befehlswarteschlange

    // Speicherreservierung und -initialisierung

    // Kerneldefinition und -ausführung

    // Synchronisierung

    return EXIT_SUCCESS;
}
    \end{minted}
    \caption{Struktur eines Alpaka-Programms}
    \label{alpaka:ueberblick:axpy:struktur}
\end{code}

\subsubsection{Beschleunigerwahl und Befehlswarteschlange}
\label{alpaka:ueberblick:axpy:queue}

Die Auswahl des Beschleunigers erfolgt bei Alpaka bereits zur Compile"=Zeit. Der
Programmierer muss also im Vorfeld eine Entscheidung darüber treffen, auf
welchen Systemen sein Programm lauffähig sein soll. Dazu wählt der Programmierer
zunächst einen der im Umfang von Alpaka vorhandenen beschleunigerspezifischen
Datentypen aus und definiert, welche Datentypen für die Angabe der im Programm
verwendeten Dimensionen und Indizes verwendet werden sollen. Dieser Vorgang ist
in Quelltext~\ref{alpaka:ueberblick:axpy:queue:acc_src} dargestellt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
// Definition für ein eindimensionales Problem
using Dim = alpaka::dim::DimInt<1u>;
using Idx = std::size_t;
using Acc = alpaka::acc::AccGpuCudaRt<Dim, Idx>;
    \end{minted}
    \caption{Auswahl der in Alpaka vorhandenen NVIDIA"=CUDA"=Implementierung}
    \label{alpaka:ueberblick:axpy:queue:acc_src}
\end{code}

In der Folge wird der definierte Typ \texttt{Acc} als Parameter für weitere
Datentypen verwendet. Bei letzteren handelt es sich um die Klassen für die
Verwaltung konkreter Beschleuniger. Diese sind in Alpaka als abstrakte
Template"=Klassen mit einer aus Sicht des Programmierers einheitlichen
Schnittstelle vorhanden. Durch den \texttt{Acc}"=Parameter werden diese Klassen
zur Compile"=Zeit spezialisiert und enthalten die für die jeweilige Hardware
korrekten Code"=Pfade. Dadurch ist der Gesamtquelltext mit wenig Aufwand
portabel, da ein einfacher Austausch des \texttt{Acc}"=Parameters passenden Code
für andere Hardware"=Architekturen generieren kann. Neben der Auswahl des
Beschleunigers ist außerdem die Auswahl eines Datentypen für den
\textit{Host} erforderlich, der später die Verwaltung einiger
\textit{Host}"=seitiger Befehle übernehmen wird. Der
Quelltext~\ref{alpaka:ueberblick:axpy:queue:dev_src} zeigt die Verwendung der
genannten Strukturen für die oben gewünschte CUDA"=Implementierung.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
using DevAcc = alpaka::dev::Dev<Acc>;
using PltfAcc = alpaka::pltf::Pltf<Acc>;
using PltfHost = alpaka::pltf::PltfCpu;
    \end{minted}
    \caption{Spezialisierung abstrakter Alpaka"=Klassen}
    \label{alpaka:ueberblick:axpy:queue:dev_src}
\end{code}

In einem weiteren Schritt muss die Befehlswarteschlange initialisiert werden.
Diese dient -- wie auch in SYCL -- als Verbindungselement zwischen \textit{Host}
und \textit{Device}. Alle das \textit{Device} betreffenden Befehle werden in
ihr eingereiht. Anders als in SYCL gibt es in Alpaka die Möglichkeit, eine zum
\textit{Host} synchrone Warteschlange zu verwenden -- der \textit{Host} wird von
der Warteschlange also so lange blockiert, bis die Verarbeitung auf dem
\textit{Device} abgeschlossen ist. In diesem Beispiel ist dies jedoch nicht
notwendig, weshalb eine asynchrone Warteschlange verwendet wird, wie
Quelltext~\ref{alpaka:ueberblick:axpy:queue:queue_src} zeigt. Wie man schnell
erkennt, existiert kein \texttt{Acc}"=Parameter für eine Alpaka"=\texttt{queue}
-- der Programmierer muss diese also ebenfalls selbst passend wählen.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
using Queue = alpaka::queue::QueueCudaRtNonBlocking;
    \end{minted}
    \caption{Auswahl der Alpaka"=Befehlswarteschlange}
    \label{alpaka:ueberblick:axpy:queue:queue_src}
\end{code}

Im letzten Schritt werden die oben definierten Datentypen instanziiert oder als
Parameter für die Instanziierung weiterer Datenstrukturen verwendet. Mit dem
in Quelltext~\ref{alpaka:ueberblick:axpy:queue:inst_src} gezeigten Vorgehen ist
der erste Abschnitt eines Alpaka"=Programms beendet.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
// instanziiere Host und Device
auto devHost = alpaka::pltf::getDevByIdx<PltfHost>(0u);
auto devAcc = alpaka::pltf::getDevByIdx<PltfAcc>(0u);

// instanziiere Befehlswarteschlange
auto queue = Queue{devAcc};
    \end{minted}
    \caption{Instanziierung der Alpaka"=Datentypen}
    \label{alpaka:ueberblick:axpy:queue:inst_src}
\end{code}

\subsubsection{Speicherreservierung und -initialisierung}
\label{alpaka:ueberblick:axpy:buffer}

Die für die AXPY"=Operation nötigen Vektoren müssen zunächst im Speicher
angelegt und initialisiert werden. Dies erfolgt in drei Stufen:

\begin{enumerate}
    \item Reserviere gleich große Speicherbereiche sowohl auf dem \textit{Host}
          als auch auf dem \textit{Device}.
    \item Initialisiere den \textit{Host}"=Speicher mit den gewünschten Werten.
    \item Kopiere die initialisierten Werte auf das \textit{Device}.
\end{enumerate}

Ähnlich wie bei SYCL werden Speicherbereiche in Alpaka durch Puffer dargestellt,
die die reinen Zeiger kapseln. Im Gegensatz zu SYCL sind die Puffer auf dem Host
und Device jedoch voneinander unabhängig. Es ist also nicht möglich, einen
gemeinsamen Puffer zu erzeugen, der die notwendigen Kopien für den Programmierer
unsichtbar im Hintergrund durchführt, wie dies in
Abschnitt~\ref{sycl:ueberblick:saxpy:buffer} für SYCL demonstriert wurde.

Für die Erzeugung von Puffern ist es zunächst nötig, die gewünschte Größe des
Puffers durch eine spezielle Datenstruktur zu definieren, wie der
Quelltext~\ref{alpaka:ueberblick:axpy:buffer:extent_src} zeigt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto extent = alpaka::vec::Vec<Dim, Idx>{numElements};
    \end{minted}
    \caption{Definition eines Größenvektors mit Alpaka}
    \label{alpaka:ueberblick:axpy:buffer:extent_src}
\end{code}

In der Folge kann die Größe verwendet werden, um den Speicher sowohl auf dem
\textit{Host} als auch auf dem \textit{Device} zu reseriveren. Dies ist in
Quelltext~\ref{alpaka:ueberblick:axpy:buffer:alloc} dargestellt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto hostBufX = alpaka::mem::buf::alloc<int, Idx>(devHost, extent);
auto hostBufY = alpaka::mem::buf::alloc<int, Idx>(devHost, extent);

auto devBufX = alpaka::mem::buf::alloc<int, Idx>(devAcc, extent);
auto devBufY = alpaka::mem::buf::alloc<int, Idx>(devAcc, extent);
    \end{minted}
    \caption{Speicherallokation mit Alpaka}
    \label{alpaka:ueberblick:axpy:buffer:alloc}
\end{code}

Die in Quelltext~\ref{alpaka:ueberblick:axpy:buffer:init} durchgeführte
Initialisierung erfolgt über die von den \textit{Host}"=Puffern gekapselten
Zeiger, auf die man bei Alpaka direkt zugreifen kann.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto hostPtrX = alpaka::mem::view::getPtrNative(hostBufX);
auto hostPtrY = alpaka::mem::view::getPtrNative(hostBufY);

for(auto i = 0; i < numElements; ++i)
{
    hostPtrX[i] = /* ... */;
    hostPtrY[i] = /* ... */;
}
    \end{minted}
    \caption{Initialisierung eines Alpaka-Puffers}
    \label{alpaka:ueberblick:axpy:buffer:init}
\end{code}

Im letzten Schritt (siehe Quelltext~\ref{alpaka:ueberblick:axpy:buffer:copy})
müssen die Daten vom \textit{Host} auf das \textit{Device} kopiert werden. Dazu
werden Kopieroperationen in der oben angelegten Warteschlange eingereiht.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
alpaka::mem::view::copy(queue, devBufX, hostBufX, extent);
alpaka::mem::view::copy(queue, devBufY, hostBufY, extent);
    \end{minted}
    \caption{Kopie der initialisierten Daten mit Alpaka}
    \label{alpaka:ueberblick:axpy:buffer:copy}
\end{code}

\subsubsection{Kerneldefinition und -ausführung}
\label{alpaka:ueberblick:axpy:kernel}

Um die parallelen Eigenschaften der Ziel"=Hardware nutzen zu können, erfordert
Alpaka eine Information, wie das zu bearbeitende Problem auf die
Hardware"=Ressourcen aufgeteilt werden soll. Dazu kann der Programmierer eine
in Alpaka vorhandene Funktion nutzen, die eine gute Aufteilung schätzen kann,
oder selbst Definitionen für die Größe des \textit{Grids}, die Zahl der
\textit{Threads} sowie der \textit{Elements} pro \textit{Thread} angeben. Die
Aufteilung in \textit{Grid}, \textit{Threads} und \textit{Elements} ist ein
Konzept der in Alpaka verwendeten Hardware"=Abstraktion und wird detailliert in
Abschnitt~\ref{alpaka:konzepte:abstraktion} behandelt. Für dieses Beispiel
genügt die Verwendung der Schätzfunktion, wie sie in
Quelltext~\ref{alpaka:ueberblick:axpy:kernel:workdiv} gezeigt wird.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto workDiv = alpaka::workdiv::getValidWorkDiv<Acc>(
                devAcc,                 // für welches Device?
                extent,                 // für welche Problemgröße?
                static_cast<Idx>(1u));  // wie viele Elemente pro Thread?
    \end{minted}
    \caption{Arbeitsaufteilung durch Alpaka"=Schätzfunktion}
    \label{alpaka:ueberblick:axpy:kernel:workdiv}
\end{code}

Alpaka"=Kernel werden in Form von C++"=Funktoren -- das heißt Strukturen mit
einem überladenen \texttt{()}"=Operator -- definiert. Der \texttt{()}"=Operator
wird mit einem speziellen Funktionsattribut versehen (\texttt{ALPAKA\_FN\_ACC}),
der im Hintergrund dafür sorgt, dass diese Funktion für das \textit{Device} und
nicht den \textit{Host} kompiliert wird.

Als Parameter nimmt der Kernel zunächst die oben definierte
\texttt{devAcc}"=Instanz entgegen. Dies ist nötig, um von dieser Struktur
gekapselte \textit{device}"=seitige Funktionen im Kernel nutzen zu können.
Daneben ist nur die Angabe der eigentlichen Funktionsparameter erforderlich,
in diesem Fall also die für den AXPY"=Algorithmus nötige Konstante \texttt{a}
sowie die Ein- bzw. Ausgangsvektoren. Wie auf dem \textit{Host} erfolgt auch auf
dem \textit{Device} der Zugriff auf die Speicherbereiche durch reine Zeiger.

Innerhalb des Kernels wird ein Index"=Raum aufgespannt (eine genaue Erläuterung
dieses Prinzips folgt in Abschnitt~\ref{alpaka:konzepte:abstraktion}), in dem
jeder ausführenden Einheit des Beschleunigers ein eindeutiger Index sowie zu
bearbeitende Elemente zugeordnet werden. Über in Alpaka vorhandene Funktionen
ist eine Orientierung innerhalb des Index"=Raums möglich, wodurch die
AXPY"=Funktion auf alle Vektor"=Elemente angewendet werden kann.

Die gesamte Kernel"=Definition ist in
Quelltext~\ref{alpaka:ueberblick:axpy:kernel:kernel_src} dargestellt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
struct AxpyKernel
{
    template <typename TAcc, typename TIdx>
    ALPAKA_FN_ACC auto operator()(
        const TAcc& acc,
        const TIdx& numElements,
        const int a,
        const int* X,
        int* Y
        ) const -> void
    {
        auto gridThreadIdx = alpaka::idx::getIdx<
                                    alpaka::Grid, alpaka::Threads>(acc)[0u];
        auto threadElemExtent = alpaka::workdiv::getWorkDiv<
                                    alpaka::Thread, alpaka::Elems>(acc)[0u];
        auto threadFirstElemIdx = gridThreadIdx * threadElemExtent;

        if(threadFirstElemIdx < numElements)
        {
            auto threadLastElemIdx = threadFirstElemIdx + threadElemExtent;
            
            for(auto i = threadFirstElemIdx; i < threadLastElemIdx; ++i)
            {
                Y[i] = a * X[i] + Y[i];
            }
        }
    }
};
    \end{minted}
    \caption{Kernel"=Definition in Alpaka}
    \label{alpaka:ueberblick:axpy:kernel:kernel_src}
\end{code}

Der so definierte Kernel wird im nächsten Schritt einem \textit{Task}
zugeordnet. Ein \textit{Task} ist prinzipiell mit einer \textit{command group}
in SYCL vergleichbar und umfasst neben der Kernel"=Definition die gewünschte
Aufteilung der Arbeit sowie die konkreten Ein- und Ausgabeparameter für eine
Kernel"=Ausführung. Ein Alpaka"=\textit{Task} wird einer Alpaka"=\textit{Queue}
übergeben und von dieser auf dem \textit{Device} zur Ausführung gebracht.

Der gesamte Prozess ist in Quelltext~\ref{alpaka:ueberblick:axpy:kernel:task}
gezeigt.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto taskKernel = alpaka::kernel::createTaskKernel<Acc>(
                    workDiv,
                    AxpyKernel{},
                    numElements,
                    a,
                    alpaka::mem::view::getPtrNative(devBufX),
                    alpaka::mem::view::getPtrNative(devBufY));

alpaka::queue::enqueue(queue, taskKernel);
    \end{minted}
    \caption{Task"=Definition und -Ausführung in Alpaka}
    \label{alpaka:ueberblick:axpy:kernel:task}
\end{code}

\subsubsection{Synchronisierung}
\label{alpaka:ueberblick:axpy:sync}

Nach dem Abschluss der Berechnung ist die Prüfung der Ergebnisse erforderlich.
Dazu werden diese durch eine Kopieroperation zunächst wieder auf den
\textit{Host} übertragen. Dies geschieht wieder in einer \textit{Queue}. Um
auf den Abschluss aller ausstehenden Operationen zu warten, kann der
\textit{Host} durch eine Wartefunktion blockiert werden. Dieser Vorgang wird in
Quelltext~\ref{alpaka:ueberblick:axpy:sync:src} demonstriert.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
alpaka::mem::view::copy(queue, hostBufX, devBufX, extent);
alpaka::mem::view::copy(queue, hostBufY, devBufY, extent);

alpaka::wait::wait(queue); // warte auf ausstehende Operationen

/* ab hier Überprüfung der Ergebnisse möglich */
    \end{minted}
    \caption{Synchronisation zwischen Host und Device in Alpaka}
    \label{alpaka:ueberblick:axpy:sync:src}
\end{code}

\subsubsection{Zusammenfassung}
\label{alpaka:ueberblick:axpy:zusammenfassung}

Der Quelltext~\ref{alpaka:ueberblick:axpy:zusammenfassung:src} zeigt das gesamte
Alpaka"=AXPY"=Beispiel, jedoch aus Platzgründen ohne die Kernel"=Definition.

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
#include <cstdlib>
#include <alpaka/alpaka.hpp>

struct AxpyKernel
{
    template <typename TAcc, typename TIdx>
    ALPAKA_FN_ACC auto operator()(const TAcc& acc, const TIdx& numElements,
                            const int a, const int* X, int* Y) const -> void
    {
        /* ... */
    }
};

auto main() -> int
{
    using Dim = alpaka::dim::DimInt<1u>;
    using Idx = std::size_t;
    using Acc = alpaka::acc::AccGpuCudaRt<Dim, Idx>;
    using DevAcc = alpaka::dev::Dev<Acc>;
    using PltfAcc = alpaka::pltf::Pltf<Acc>;
    using PltfHost = alpaka::pltf::PltfCpu;
    using Queue = alpaka::queue::QueueCudaRtNonBlocking;

    auto devHost = alpaka::pltf::getDevByIdx<PltfHost>(0u);
    auto devAcc = alpaka::pltf::getDevByIdx<PltfAcc>(0u);
    auto queue = Queue{devAcc};
    auto extent = alpaka::vec::Vec<Dim, Idx>{numElements};
    auto hostBufX = alpaka::mem::buf::alloc<int, Idx>(devHost, extent);
    auto hostBufY = alpaka::mem::buf::alloc<int, Idx>(devHost, extent);
    auto devBufX = alpaka::mem::buf::alloc<int, Idx>(devAcc, extent);
    auto devBufY = alpaka::mem::buf::alloc<int, Idx>(devAcc, extent);
    auto hostPtrX = alpaka::mem::view::getPtrNative(hostBufX);
    auto hostPtrY = alpaka::mem::view::getPtrNative(hostBufY);

    for(auto i = 0; i < numElements; ++i) {
        hostPtrX[i] = /* ... */;
        hostPtrY[i] = /* ... */;
    }
    alpaka::mem::view::copy(queue, devBufX, hostBufX, extent);
    alpaka::mem::view::copy(queue, devBufY, hostBufY, extent);

    auto workDiv = alpaka::workdiv::getValidWorkDiv<Acc>(
                    devAcc, extent, static_cast<Idx>(1u));
    auto taskKernel = alpaka::kernel::createTaskKernel<Acc>(
                        workDiv, AxpyKernel{}, numElements, a,
                        alpaka::mem::view::getPtrNative(devBufX),
                        alpaka::mem::view::getPtrNative(devBufY));
    alpaka::queue::enqueue(queue, taskKernel);
    alpaka::mem::view::copy(queue, hostBufX, devBufX, extent);
    alpaka::mem::view::copy(queue, hostBufY, devBufY, extent);
    alpaka::wait::wait(queue);

    return EXIT_SUCCESS;
}
    \end{minted}
    \caption{Vollständiges Alpaka-AXPY-Beispiel}
    \label{alpaka:ueberblick:axpy:zusammenfassung:src}
\end{code}

\section{Weiterführende Konzepte}\label{alpaka:konzepte}

\subsection{Hardware"=Abstraktion}
\label{alpaka:konzepte:abstraktion}

\subsection{Abhängigkeiten zwischen Kerneln}
\label{alpaka:konzepte:abhaengigkeiten}

\subsection{Fehlerbehandlung}

\subsection{Profiling}

Im Gegensatz zu SYCL bringt Alpaka keine eigenen Werkzeuge für das Profiling
mit. Da Alpaka zur Compile"=Zeit auf die herstellerspezifischen Schnittstellen
abgebildet wird, ist dies auch nicht nötig -- dem Programmierer stehen so die
vom jeweiligen Hardware"=Hersteller mitgelieferten Profiling"=Werkzeuge zur
Verfügung. So lässt sich beispielsweise der CUDA"=Profiler \texttt{nvprof} für
das Profiling eines mit Alpaka auf NVIDIA"=Hardware portierten Programms
verwenden, was mit den für NVIDIA"=GPUs existierenden SYCL"=Implementierungen
nicht möglich ist.
